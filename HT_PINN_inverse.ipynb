{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "PINN_multi_pump_test_for_K.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QuanGuo/HT-PINN/blob/main/HT_PINN_inverse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93DG5Y_upzAr"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "\n",
        "# import os\n",
        "# os.chdir('/content/gdrive/My Drive/Colab Notebooks/')\n",
        "# !pwd\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# from torch.autograd import Variable, grad\n",
        "# import matplotlib as mpl\n",
        "# import matplotlib.pyplot as plt\n",
        "# from utils import *\n",
        "\n",
        "# import time\n",
        "# import ast\n",
        "# import copy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgRucU9lAK6J"
      },
      "source": [
        "################ colorbar type for plot ##############\n",
        "cmap5 = [ 'flag', 'prism', 'ocean', 'gist_earth', 'terrain', 'gist_stern',\n",
        "'gnuplot', 'gnuplot2', 'CMRmap', 'cubehelix', 'brg',\n",
        "'gist_rainbow', 'rainbow', 'jet', 'turbo', 'nipy_spectral',\n",
        "'gist_ncar']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnsywHTMpzA5"
      },
      "source": [
        "class PhysicsInformedNN(nn.Module):\n",
        "\n",
        "  def __init__(self, layers_u, layers_K, input_K=None, inv_params=None, num_pumps=25):\n",
        "    super(PhysicsInformedNN, self).__init__()\n",
        "  \n",
        "    self.layers = layers\n",
        "  \n",
        "    self.weights_u, self.biases_u = [], []\n",
        "    self.weights_K, self.biases_K = self.initialize_NN(layers_K)   \n",
        "    self.weights = []\n",
        "    self.biases = []\n",
        "\n",
        "    self.preds = None\n",
        "\n",
        "    self.loss = 0.0\n",
        "\n",
        "    self.loss_list = []   \n",
        "    # self.loss_dict = {'neum':[], 'diri':[],'u':[],'f':[],'K':[],'around':[],'pump':[]}\n",
        "    self.loss_container = []\n",
        "    for i in range(num_pumps): \n",
        "      loss_dict = {'neum':[0.0], 'diri':[0.0],'u':[0.0],'f':[0.0],'K':[0.0],'around':[0.0],'pump':[0.0]}\n",
        "      self.loss_container.append(loss_dict)\n",
        "      w, b = self.initialize_NN(layers_u)\n",
        "      self.weights_u.append(w)\n",
        "      self.biases_u.append(b)\n",
        "    \n",
        "      self.weights += w\n",
        "      self.biases += b\n",
        "    self.weights += self.weights_K\n",
        "    self.biases += self.biases_K\n",
        "\n",
        "  def create_dK(self, K, dx, dy):\n",
        "    dK_cen = (K[:,2:] - K[:,0:-2])/dx/2\n",
        "    dK_left = (K[:,1:2] - K[:,0:1])/dx\n",
        "    dK_right = (K[:,-1:] - K[:,-2:-1])/dx\n",
        "    dKdx = np.hstack((dK_left, dK_cen, dK_right))\n",
        "\n",
        "    dK_mid = (K[2:] - K[0:-2,])/dy/2\n",
        "    dK_top = (K[1:2] - K[0:1])/dy\n",
        "    dK_bot = (K[-1:] - K[-2:-1])/dy\n",
        "    dKdy = np.vstack((dK_top, dK_mid, dK_bot))\n",
        "    \n",
        "    dKdx = torch.tensor(dKdx, requires_grad=True)\n",
        "    dKdy = torch.tensor(dKdy, requires_grad=True)\n",
        "    return dKdx, dKdy\n",
        "\n",
        "  def initialize_NN(self, layers):        \n",
        "    weights = []\n",
        "    biases = []\n",
        "    num_layers = len(layers) \n",
        "    for l in range(0,num_layers-1):\n",
        "      W = self.xavier_init(size=[layers[l], layers[l+1]])\n",
        "      b = Variable(torch.zeros([1,layers[l+1]], dtype=torch.float32), requires_grad=True)\n",
        "      weights.append(W)\n",
        "      biases.append(b)        \n",
        "    return weights, biases\n",
        "      \n",
        "  def xavier_init(self, size):\n",
        "    return Variable(nn.init.xavier_normal_(torch.empty(size[0], size[1])), requires_grad=True)\n",
        "    \n",
        "  def load_NN(self, layers, params):\n",
        "    weights = []\n",
        "    biases = []\n",
        "    params = torch.tensor(params,dtype=torch.float32)\n",
        "    num_layers = len(layers)\n",
        "    i = 1\n",
        "    for l in range(0,num_layers-2):\n",
        "      W = torch.zeros([layers[l], layers[l+1]], dtype=torch.float32)\n",
        "      b = torch.zeros([1,layers[l+1]], dtype=torch.float32)\n",
        "      W = W + params[i:i+layers[l]]\n",
        "      b = b + params[i+layers[l]]\n",
        "      i = i + layers[l] + 1\n",
        "      weights.append(Variable(W, requires_grad=True))\n",
        "      biases.append(Variable(b, requires_grad=True))\n",
        "\n",
        "    output_layer_w = Variable(params[i][:,None], requires_grad=True)\n",
        "    \n",
        "    output_layer_b = torch.zeros((1,1), dtype=torch.float32)\n",
        "    output_layer_b = output_layer_b + params[i+1,0]\n",
        "\n",
        "    weights.append(output_layer_w)\n",
        "    biases.append(Variable(output_layer_b,requires_grad=True))\n",
        "\n",
        "    return weights, biases\n",
        "\n",
        "  def neural_net(self, x, y, weights, biases):\n",
        "\n",
        "    num_layers = len(weights) + 1\n",
        "    H = torch.cat((x,y),1)\n",
        "    for l in range(0,num_layers-2):\n",
        "      W = weights[l]\n",
        "      b = biases[l]\n",
        "      H = torch.tanh(torch.add(torch.matmul(H, W), b))\n",
        "\n",
        "    W = weights[-1]\n",
        "    b = biases[-1]\n",
        "    Y = torch.add(torch.matmul(H, W), b) #.requires_grad_()\n",
        "\n",
        "    return Y\n",
        "\n",
        "  def neural_net_sigmoid(self, x, y, weights, biases):\n",
        "\n",
        "    num_layers = len(weights) + 1\n",
        "    H = torch.cat((x,y),1)\n",
        "    for l in range(0,num_layers-2):\n",
        "      W = weights[l]\n",
        "      b = biases[l]\n",
        "      H = torch.sigmoid(torch.add(torch.matmul(H, W), b))\n",
        "\n",
        "    W = weights[-1]\n",
        "    b = biases[-1]\n",
        "    Y = torch.add(torch.matmul(H, W), b) #.requires_grad_()\n",
        "\n",
        "    return Y\n",
        "\n",
        "  def neural_net_relu(self, x, y, weights, biases):\n",
        "    p = torch.tensor(0.001)\n",
        "    num_layers = len(weights) + 1\n",
        "    H = torch.cat((x,y),1)\n",
        "\n",
        "    W = weights[0]\n",
        "    b = biases[0]\n",
        "    H = nn.functional.relu(torch.add(torch.matmul(H, W), b))\n",
        "\n",
        "    for l in range(1,num_layers-2):\n",
        "      W = weights[l]\n",
        "      b = biases[l]\n",
        "      H = torch.tanh(torch.add(torch.matmul(H, W), b))\n",
        "    W = weights[-1]\n",
        "    b = biases[-1]\n",
        "    # Y = nn.functional.relu(torch.add(torch.matmul(H, W), b))\n",
        "    # Y = nn.functional.prelu(torch.add(torch.matmul(H, W), b),p)+1\n",
        "    Y = torch.add(torch.matmul(H, W), b)\n",
        "\n",
        "    return Y\n",
        "\n",
        "  def net_u(self, x, y, weights, biases): # head u, including Dirichlet BCs\n",
        "    u = self.neural_net(x, y, weights, biases)\n",
        "    return u\n",
        "  \n",
        "  def net_K(self, x, y): # hydraulic conductivity K\n",
        "    K = self.neural_net(x, y, self.weights_K, self.biases_K)\n",
        "    return K\n",
        "  \n",
        "  def get_loc_idx(self, x, y): # hydraulic conductivity K\n",
        "    x = x.detach().numpy()\n",
        "    y = y.detach().numpy()\n",
        "\n",
        "    x_arr = np.linspace(-1,1,self.given_K.shape[0])\n",
        "    y_arr = np.linspace(-1,1,self.given_K.shape[1])\n",
        "\n",
        "    xsorted = np.argsort(x_arr)\n",
        "    xpos = np.searchsorted(x_arr[xsorted], x)\n",
        "    r = xsorted[xpos]\n",
        "\n",
        "    ysorted = np.argsort(y_arr)\n",
        "    ypos = np.searchsorted(y_arr[ysorted], y)\n",
        "    c = xsorted[ypos]\n",
        "    # K = given_K[r, c]\n",
        "\n",
        "    return (r, c)\n",
        "\n",
        "\n",
        "  def net_du(self, x, y, weights, biases): # first-order derivative match, inlcuding Neumann BCs\n",
        "\n",
        "    u = self.net_u(x, y, weights, biases)#, self.weights_u, self.biases_u)\n",
        "\n",
        "    u_x = grad(u.sum(), x, create_graph=True, retain_graph=True)[0]\n",
        "    u_y = grad(u.sum(), y, create_graph=True)[0]\n",
        "\n",
        "    return u_x.requires_grad_(True), u_y.requires_grad_(True)\n",
        "\n",
        "  def net_dK(self, x, y): # first-order derivative of K\n",
        "    K = self.net_K(x, y)#, self.weights_u, self.biases_u)\n",
        "\n",
        "    K_x = grad(K.sum(), x, create_graph=True)[0]\n",
        "    K_y = grad(K.sum(), y, create_graph=True)[0]\n",
        "\n",
        "    return K_x.requires_grad_(True), K_y.requires_grad_(True)\n",
        "\n",
        "\n",
        "  def net_f(self, x, y, weights, biases): # general PDE match, usually formulated in higher-order\n",
        "\n",
        "    u_x, u_y = self.net_du(x, y, weights, biases)\n",
        "    u_yy = grad(u_y.sum(), y, create_graph=True)[0]\n",
        "    u_xx = grad(u_x.sum(), x, create_graph=True)[0]\n",
        "\n",
        "    K = self.net_K(x, y)\n",
        "    K_x, K_y = self.net_dK(x, y)\n",
        "    # K_x = grad(K.sum(), x, create_graph=True)[0]\n",
        "    # K_y = grad(K.sum(), y, create_graph=True)[0]\n",
        "\n",
        "    f = K*(u_yy + u_xx) + K_x*u_x + K_y*u_y\n",
        "\n",
        "    return f.requires_grad_(True)\n",
        "\n",
        "  def forward(self, x_tensors, y_tensors, weights, biases, keys=None):\n",
        "\n",
        "    if keys is None:\n",
        "      keys = x_tensors.keys()\n",
        "    else:\n",
        "      preds = dict()\n",
        "      for i in keys:\n",
        "          preds[i] = None\n",
        "\n",
        "    for i in keys:\n",
        "\n",
        "      if i == 'neum':\n",
        "        dudx_pred, dudy_pred = self.net_du(x_tensors[i], y_tensors[i], weights, biases)\n",
        "        # flux_pred = self.net_f(x_tensors[i], y_tensors[i])\n",
        "        preds[i] = dudy_pred\n",
        "\n",
        "      elif i == 'f':\n",
        "        f_pred = self.net_f(x_tensors[i], y_tensors[i], weights, biases)\n",
        "        preds[i] = f_pred\n",
        "\n",
        "      elif i == 'u':\n",
        "        u_pred = self.net_u(x_tensors[i], y_tensors[i], weights, biases) \n",
        "        preds[i] = u_pred\n",
        "          \n",
        "      elif i == 'K':\n",
        "        # K_pred = nn.functional.relu(self.net_K(x_tensors[i], y_tensors[i]))+1e-4\n",
        "        K_pred = self.net_K(x_tensors[i], y_tensors[i])\n",
        "\n",
        "        preds[i] = K_pred\n",
        "          \n",
        "      elif i == 'diri':\n",
        "        diri_pred = self.net_u(x_tensors[i], y_tensors[i], weights, biases) \n",
        "        preds[i] = diri_pred\n",
        "\n",
        "      elif i == 'pump':\n",
        "        p_pred = self.net_f(x_tensors[i], y_tensors[i], weights, biases)\n",
        "        preds[i] = p_pred\n",
        "\n",
        "    return preds\n",
        "\n",
        "  def loss_func(self, pred_dict, true_dict, pump_id, weights=None):\n",
        "  \n",
        "    loss = torch.tensor(0.0, dtype=torch.float32)\n",
        "    keys = pred_dict.keys()\n",
        "\n",
        "    if weights is None:\n",
        "      weights = dict()\n",
        "      for i in keys:\n",
        "        weights[i] = 1.0\n",
        "\n",
        "    for i in keys:\n",
        "      res = pred_dict[i] - true_dict[i]\n",
        "      loss += weights[i]*torch.mean(res.pow(2))\n",
        "      r = torch.mean(res.pow(2)).item()\n",
        "      self.loss_container[pump_id][i].append(r*weights[i])\n",
        "    return loss.requires_grad_()\n",
        "  \n",
        "  def customized_backward(self, loss, params):\n",
        "    grads = grad(loss, params, retain_graph=True)\n",
        "    for vid in range(len(params)):\n",
        "      params[vid].grad = grads[vid]\n",
        "    return grads\n",
        "\n",
        "  def unzip_train_dict(self, train_dict, keys=None):\n",
        "    if keys is None:\n",
        "      keys = train_dict.keys()\n",
        "\n",
        "    x_tensors = dict()\n",
        "    y_tensors = dict()\n",
        "    true_dict = dict()\n",
        "\n",
        "    for i in keys:\n",
        "      x_tensors[i] = train_dict[i][0]\n",
        "      y_tensors[i] = train_dict[i][1]\n",
        "      true_dict[i] = train_dict[i][2]\n",
        "\n",
        "    return (x_tensors, y_tensors, true_dict)\n",
        "\n",
        "  def train(self, epoch, data_batch, loss_func, optimizer, pred_keys=None, loss_weights=None, pump_id_list=[0], print_interval=1000):\n",
        "      \n",
        "    if pred_keys is None:\n",
        "      pred_keys= data_batch[0].keys()\n",
        "    start_time = time.time()\n",
        "    for i in range(epoch):\n",
        "      optimizer.zero_grad()\n",
        "      loss = 0.0\n",
        "      for pump_id in pump_id_list:\n",
        "        train_dict = data_batch[pump_id]\n",
        "\n",
        "        (x_tensors, y_tensors, true_dict) = self.unzip_train_dict(train_dict,pred_keys)\n",
        "        pred_dict = self.forward(x_tensors, y_tensors, self.weights_u[pump_id], self.biases_u[pump_id], keys=pred_keys)\n",
        "        loss += loss_func(pred_dict, true_dict, pump_id, loss_weights)\n",
        "\n",
        "      loss.backward()\n",
        "      self.callback(loss.detach().numpy().squeeze())\n",
        "\n",
        "      if np.remainder(len(self.loss_list),print_interval) == 1:\n",
        "        elapsed = time.time() - start_time\n",
        "        print('Iter # %d, Loss: %.8f, Time: %.4f' % (len(self.loss_list), self.loss_list[-1], elapsed))\n",
        "        print_loss = dict()\n",
        "        for pid in pump_id_list:\n",
        "          print_loss = \"Pump \"+ str(pid) + \": \"\n",
        "          for k in ['u','f','K','neum','pump','diri']:\n",
        "            s = k+\":\"+str(self.loss_container[pid][k][-1])+\"; \"\n",
        "            print_loss += s\n",
        "          print(print_loss)\n",
        "        start_time = time.time()  \n",
        "      \n",
        "      # g = self.customized_backward(loss, self.weights+self.biases)\n",
        "      optimizer.step()\n",
        "\n",
        "    # self.pred_dict = self.forward(x_tensors, y_tensors, keys=pred_keys)\n",
        "    # self.loss = loss_func(self.pred_dict, true_dict) #.requires_grad_()\n",
        "\n",
        "  def callback(self, loss):\n",
        "    self.loss_list.append(loss)\n",
        "\n",
        "  def coor_shift(self, X, lbs, ubs):\n",
        "    return 2.0*(X - lbs) / (ubs - lbs) - 1\n",
        "\n",
        "  def data_loader(self, X, u, lbs, ubs):\n",
        "              \n",
        "    X = self.coor_shift(X, lbs, ubs)\n",
        "\n",
        "    x_tensor = torch.tensor(X[:,0:1], requires_grad=True, dtype=torch.float32)\n",
        "    y_tensor = torch.tensor(X[:,1:2], requires_grad=True, dtype=torch.float32)\n",
        "\n",
        "    u_tensor = torch.tensor(u, dtype=torch.float32)\n",
        "    \n",
        "    return (x_tensor, y_tensor, u_tensor)\n",
        "\n",
        "  def predict(self, X_input, pid=0, target='u'):\n",
        "    x_tensor = torch.tensor(X_input[:,0:1], dtype=torch.float32, requires_grad=True)\n",
        "    y_tensor = torch.tensor(X_input[:,1:2], dtype=torch.float32, requires_grad=True)\n",
        "    w = self.weights_u[pid]\n",
        "    b = self.biases_u[pid]\n",
        "    pred = None\n",
        "    if target == 'u':\n",
        "      pred = self.net_u(x_tensor, y_tensor, w, b).detach().numpy().squeeze()\n",
        "    elif target == 'du':\n",
        "      dudx, dudy = self.net_du(x_tensor, y_tensor, w, b)\n",
        "      return dudx.detach().numpy().squeeze(), dudy.detach().numpy().squeeze()\n",
        "    elif target == 'f':\n",
        "      pred = self.net_f(x_tensor, y_tensor, w, b).detach().numpy().squeeze()\n",
        "\n",
        "    elif target == 'K':\n",
        "      pred = self.net_K(x_tensor, y_tensor).detach().numpy().squeeze()\n",
        "\n",
        "    return pred\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lByzu7hKhZ26"
      },
      "source": [
        "# create PINN instance with certain number of LAYERS and HIDDEN UNITS\n",
        "##################### read lnK realizations (~ 10 fields) ##################\n",
        "save_label = '0720'   # generation date\n",
        "field_id = 1   # usually 12 fields: id -> [0-11]\n",
        "\n",
        "#id=0 for date 0722 with max depth=0.16\n",
        "#id=1 for date 0720 with max depth=0.16\n",
        "#id=1 for date 0724 with max depth=3, 128x128\n",
        "save_path = './data/'+save_label+'/field_'+str(field_id)\n",
        "isDir = os.path.isdir(save_path)\n",
        "\n",
        "if isDir:\n",
        "  print('/field_'+str(field_id)+' existing')\n",
        "  # file = open(save_path+\"hyper_parameters.txt\", \"r\")\n",
        "  # hyper_param = ast.literal_eval(file.read())\n",
        "  # file.close()\n",
        "  # print(hyper_param)\n",
        "else:\n",
        "  print('/field_'+str(field_id)+' does not exist') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNqP6885NZUF"
      },
      "source": [
        "######## first line contains geostatistical and geometry parameters #########\n",
        "field_file_name = './data/' + save_label +'/fields_'+save_label+'.csv'\n",
        "df = pd.read_csv(field_file_name, sep=',', header=None)\n",
        "mu = df.values[0][0]\n",
        "sigma2 = df.values[0][1]\n",
        "lx = df.values[0][2]\n",
        "ly = df.values[0][3]\n",
        "k = int(df.values[0][4])\n",
        "nx = int(df.values[0][5])\n",
        "ny = int(df.values[0][6])\n",
        "\n",
        "if int(df.values[0][7]) == 1:\n",
        "  Ctype = \"Exponential\"\n",
        "else:\n",
        "  Ctype = \"Gaussian\"\n",
        "\n",
        "print(\"Covariance func: \" + Ctype)\n",
        "print(\"mu(lnK):\", mu, \" m/s\")\n",
        "print(\"variance:\", sigma2)\n",
        "print(\"lx:\", lx, \"; ly:\", ly)\n",
        "print(\"Resolution: \" + str(nx) + \"x\" + str(ny))\n",
        "\n",
        "if df.values[0].shape[0] > 11:\n",
        "  Q = float(df.values[0][8])\n",
        "  dt_real = float(df.values[0][9])\n",
        "  dx_real = float(df.values[0][10])\n",
        "  dy_real = float(df.values[0][11])\n",
        "  print(\"Q:\", str(Q), \" m3/s\")\n",
        "  print(\"cell size (dx x dy): \" + str(dx_real) + \"m x \" + str(dy_real) + \"m\")\n",
        "  print(\"Time unit:\", str(dt_real), \" s\")\n",
        "elif df.values[0].shape[0] > 8:\n",
        "  Q = float(df.values[0][8])\n",
        "  dx_real = float(df.values[0][9])\n",
        "  dy_real = float(df.values[0][10])\n",
        "  print(\"Q:\", str(Q), \" m3/s\")\n",
        "  print(\"cell size (dx x dy): \" + str(dx_real) + \"m x \" + str(dy_real) + \"m\")\n",
        "\n",
        "######## select lnK field and reshape ############\n",
        "logK = df.values[1:,field_id]\n",
        "logK = logK.reshape((nx,ny)).T\n",
        "K = np.exp(logK)\n",
        "\n",
        "#### pump test results on the field ##########\n",
        "head_file_name = './data/'+save_label+'/heads_'+save_label+'_'+str(field_id+1)+'.csv'\n",
        "df2=pd.read_csv(head_file_name, sep=',',header=None)\n",
        "heads = df2.values\n",
        "num_wells = heads.shape[1]\n",
        "\n",
        "# solution from FEM is on node and has resolution (nx+1) x (ny+1)\n",
        "# interpolate center value on each cell as average on 4 corner nodes\n",
        "heads_at_cell = np.zeros((nx*ny,num_wells))\n",
        "for ii in range(num_wells):\n",
        "  head = heads[:,ii]\n",
        "  head = head.reshape((nx+1,ny+1))\n",
        "  head = head[1:,:]+head[0:-1,:]\n",
        "  head = head[:,1:]+head[:,0:-1]\n",
        "  head = head/4.0\n",
        "  heads_at_cell[:,ii]=head.T.flatten()\n",
        "\n",
        "#### read variables ufor inverse ##########\n",
        "cov_file_name = './data/' + save_label +'/sign_'+save_label+'.csv'\n",
        "df3=pd.read_csv(cov_file_name, sep=',',header=None)\n",
        "signs = df3.values[:,0].T   # sign of first variables of each eigenvector\n",
        "alpha = df3.values[:,field_id+1][:,None] # r.v. used in PCA decomposition\n",
        "\n",
        "np.savetxt(\"logK_field.txt\",logK)\n",
        "np.savetxt(\"alpha_vector.txt\",alpha)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0X6dTtkozwkQ"
      },
      "source": [
        "####### plot certain head tomography on 3D surf\n",
        "\n",
        "# for pid in range(num_wells):\n",
        "#   idx = np.argmin(heads[:,pid])\n",
        "#   heads[idx,pid] = 0.5*(heads[idx-1,pid]+heads[idx+1,pid])\n",
        "\n",
        "si = 2\n",
        "\n",
        "x = np.linspace(0,1,nx+1)\n",
        "y = np.linspace(0,1,ny+1)\n",
        "X,Y = np.meshgrid(x,y)\n",
        "\n",
        "head = heads[:,si].reshape((nx+1,ny+1))\n",
        "\n",
        "fig = plt.figure(figsize=(12,5))\n",
        "\n",
        "# set up the axes for the first plot\n",
        "ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
        "surf = ax.plot_surface(X, Y, head, rstride=1, cstride=1, cmap=cm.viridis,\n",
        "                          linewidth=0, antialiased=False)\n",
        "\n",
        "ax.set_title('FEM')\n",
        "ax.view_init(30,45)\n",
        "ax.set_xlabel('$x$')\n",
        "ax.set_ylabel('$y$')\n",
        "ax.set_zlabel('$z$')\n",
        "\n",
        "x = np.linspace(0,1,nx)\n",
        "y = np.linspace(0,1,ny)\n",
        "X,Y = np.meshgrid(x,y)\n",
        "Exact = heads_at_cell[:,si].reshape((nx,ny)).T\n",
        "\n",
        "ax2 = fig.add_subplot(1, 2, 2, projection='3d')\n",
        "surf = ax2.plot_surface(X, Y, Exact, rstride=1, cstride=1, cmap=cm.viridis,\n",
        "                          linewidth=0, antialiased=False)\n",
        "ax2.set_title('Exact')\n",
        "ax2.set_xlabel('$x$')\n",
        "ax2.set_ylabel('$y$')\n",
        "ax2.set_zlabel('$z$')\n",
        "\n",
        "############### define domain with (0,0) at center ######\n",
        "Lox, Loy = 1, 1\n",
        "dx, dy = Lox/nx, Loy/ny\n",
        "\n",
        "x = np.arange((-Lox/2+dx/2),(Lox/2),dx)\n",
        "y = np.arange((-Lox/2+dx/2),(Lox/2),dy)\n",
        "\n",
        "X, Y = np.meshgrid(x,y)\n",
        "\n",
        "X_star = np.hstack((X.flatten()[:,None], Y.flatten()[:,None]))\n",
        "\n",
        "################### f PDE (2nd-order derivatives) plot: 2D & 3D ##################\n",
        "dx,dy=1/nx,1/ny\n",
        "du_true_cen = (Exact[:,2:] - Exact[:,0:-2])/dx/2\n",
        "du_true_left = (Exact[:,1:2] - Exact[:,0:1])/dx\n",
        "du_true_right = (Exact[:,-1:] - Exact[:,-2:-1])/dx\n",
        "du_true_dx = np.hstack((du_true_left, du_true_cen, du_true_right))\n",
        "\n",
        "du_true_mid = (Exact[2:] - Exact[0:-2,])/dy/2\n",
        "du_true_top = (Exact[1:2] - Exact[0:1])/dy\n",
        "du_true_bot = (Exact[-1:] - Exact[-2:-1])/dy\n",
        "du_true_dy = np.vstack((du_true_top, du_true_mid, du_true_bot))\n",
        "\n",
        "du2_cen = ((Exact[:,2:] - Exact[:,1:-1]) - (Exact[:,1:-1] - Exact[:,0:-2]))/dx**2\n",
        "du2_left = du2_cen[:,0][:,None]\n",
        "du2_right = du2_cen[:,-1][:,None]\n",
        "du2dx = np.hstack((du2_left, du2_cen, du2_right))\n",
        "\n",
        "du2_mid = ((Exact[2:] - Exact[1:-1]) - (Exact[1:-1] - Exact[0:-2]))/dy**2\n",
        "du2_top = du2_mid[0]\n",
        "du2_bot = du2_mid[-1]\n",
        "du2dy = np.vstack((du2_top, du2_mid, du2_bot))\n",
        "\n",
        "dK_true_cen = (K[:,2:] - K[:,0:-2])/dx/2\n",
        "dK_true_left = (K[:,1:2] - K[:,0:1])/dx\n",
        "dK_true_right = (K[:,-1:] - K[:,-2:-1])/dx\n",
        "dK_true_dx = np.hstack((dK_true_left, dK_true_cen, dK_true_right))\n",
        "\n",
        "dK_true_mid = (K[2:] - K[0:-2,])/dy/2\n",
        "dK_true_top = (K[1:2] - K[0:1])/dy\n",
        "dK_true_bot = (K[-1:] - K[-2:-1])/dy\n",
        "dK_true_dy = np.vstack((dK_true_top, dK_true_mid, dK_true_bot))\n",
        "\n",
        "f_res = np.multiply(dK_true_dx,du_true_dx) + np.multiply(dK_true_dy, du_true_dy) + np.multiply(du2dy+du2dx,K)\n",
        "\n",
        "fig2 = plt.figure(figsize=(12,5))\n",
        "\n",
        "ax4 = fig2.add_subplot(1, 2, 1)\n",
        "ax4.pcolor(X,Y,f_res)\n",
        "\n",
        "ax3 = fig2.add_subplot(1, 2, 2, projection='3d')\n",
        "surf = ax3.plot_surface(X, Y, f_res, rstride=1, cstride=1, cmap=cm.viridis,\n",
        "                          linewidth=0, antialiased=False)\n",
        "ax3.set_xlabel('$x$')\n",
        "ax3.set_ylabel('$y$')\n",
        "ax3.set_zlabel('$f$')\n",
        "\n",
        "print(np.max(f_res))\n",
        "print(np.min(f_res))\n",
        "print(3600/1.25/1.25*0.001)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D98vQHiBPC3i"
      },
      "source": [
        "#################  K measurement cell id ##################\n",
        "# dtb: dist (in num of cells) to boundary, e.g., 3, 4, 5..\n",
        "# dtm: dist (in ratio) between nearest measurement, e.g., 1/8, 1/4 ..\n",
        "##  resolution    dtb     dtm       shift\n",
        "##   32x32         3      1/6         0 \n",
        "##   32x32         3      1/7         1 \n",
        "##   51x51         3      1/6         4\n",
        "##   64x64         3      1/6         4 \n",
        "##   64x64         3      1/7         2\n",
        "\n",
        "# dtb = 1\n",
        "\n",
        "# dtm = 1/6.0\n",
        "# if nx == 32:\n",
        "#   shift = 1\n",
        "# else:\n",
        "#   shift = 0\n",
        "\n",
        "# xloc = np.arange(dtb, nx-dtb, int((nx-2*dtb)*dtm)) + shift\n",
        "# yloc = np.arange(dtb, nx-dtb, int((ny-2*dtb)*dtm)) + shift\n",
        "\n",
        "# xloc = np.repeat(xloc,len(xloc))\n",
        "# yloc = np.tile(yloc,len(yloc))\n",
        "\n",
        "# # xloc = np.floor(np.linspace(1, nx-2, 7)).astype(int)\n",
        "# # yloc = np.floor(np.linspace(1, ny-2, 7)).astype(int)\n",
        "\n",
        "# # xloc, yloc = np.repeat(xloc,len(yloc)), np.tile(yloc,len(xloc))\n",
        "\n",
        "# K_measure_id = xloc + yloc*ny\n",
        "\n",
        "dtb = 1\n",
        "\n",
        "dtm = 1/5.0\n",
        "if nx == 32:\n",
        "  shift = 1\n",
        "else:\n",
        "  shift = 0\n",
        "  # shift = 1\n",
        "\n",
        "xloc = np.arange(dtb, nx-dtb, int((nx-2*dtb)*dtm)) + shift\n",
        "yloc = np.arange(dtb, nx-dtb, int((ny-2*dtb)*dtm)) + shift\n",
        "# print(xloc)\n",
        "xloc, yloc = np.repeat(xloc,len(yloc)), np.tile(yloc,len(xloc))\n",
        "K_measure_id = xloc + yloc*ny\n",
        "\n",
        "# 128x128: dtb = 14~15\n",
        "# 256x256: dtb = 28~29\n",
        "dtb=14\n",
        "xloc2 = np.floor(np.linspace(dtb, nx-dtb-1, 5)).astype(int)\n",
        "yloc2 = np.floor(np.linspace(dtb, ny-dtb-1, 5)).astype(int)\n",
        "print(xloc2)\n",
        "\n",
        "xloc2, yloc2 = np.repeat(xloc2,len(yloc2)), np.tile(yloc2,len(xloc2))\n",
        "K_extra_id = xloc2 + yloc2*ny\n",
        "\n",
        "\n",
        "K_measure_id = np.hstack((K_measure_id,K_extra_id))\n",
        "print(len(K_measure_id))\n",
        "\n",
        "\n",
        "# N_K = 40\n",
        "# K_measure_id = np.random.choice(nx*ny, N_K, replace=False)\n",
        "\n",
        "# Km_id = np.loadtxt(save_path+\"/npump_9/K_measure_id_49.txt\").astype(int)\n",
        "# print(Km_id)\n",
        "# K_measure_id = Km_id\n",
        "################ K measurments well ###################\n",
        "\n",
        "fig2,ax2 = plt.subplots(figsize=(7,6))\n",
        "\n",
        "#cmap5: 13 \"jet\"\n",
        "im2 = ax2.pcolor(X,Y,logK, cmap='jet')\n",
        "# plt.colorbar()\n",
        "ax2.set_xlabel('x')\n",
        "ax2.set_ylabel('y')\n",
        "fig.colorbar(im2, ax=ax2, orientation='vertical')\n",
        "ax2.scatter(X_star[K_measure_id,0], X_star[K_measure_id,1], marker='v', zorder=1, alpha= 1, c='m', s=30)\n",
        "ax2.set_title(\"lnK field\")\n",
        "print(K_measure_id)\n",
        "# ############### load K_measure_id ################\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y59cNwFEtPol"
      },
      "source": [
        "#################  well network cell id ##################\n",
        "well_id = np.arange(num_wells)\n",
        "\n",
        "# pump well matrix-wise index (index on x and y axis)\n",
        "xloc = np.arange(int(nx/4),int(nx*3/4+1),int(nx/8))\n",
        "yloc = np.arange(int(ny/4),int(ny*3/4+1),int(ny/8))\n",
        "\n",
        "xloc = np.repeat(xloc,5)\n",
        "yloc = np.tile(yloc,5)\n",
        "\n",
        "# pump well index\n",
        "pump_cell_idx = xloc + yloc*ny\n",
        "\n",
        "#################  boundary cell id ##################\n",
        "id1 = np.where(X.flatten() == min(x))\n",
        "id2 = np.where(X.flatten() == max(x))\n",
        "\n",
        "id3 = np.where(Y.flatten() == min(y))\n",
        "id4 = np.where(Y.flatten() == max(y))\n",
        "xbound_idx = np.unique(np.hstack((id1,id2)))\n",
        "ybound_idx = np.unique(np.hstack((id3,id4)))\n",
        "\n",
        "#################  relax region cell id ##################\n",
        "# points around pump (relax region for PDE or extra monitor cells)\n",
        "r = 3\n",
        "pump_row_idx = np.repeat(pump_cell_idx[:,None],2*r-1,1)\n",
        "pump_row_idx = pump_row_idx - np.arange(-r+1,r)\n",
        "around_idx = [pump_row_idx]\n",
        "for i in range(1,r):\n",
        "  around_idx = [pump_row_idx-nx*i] + around_idx + [nx*i+pump_row_idx]\n",
        "around_idx = np.hstack(around_idx)\n",
        "around_idx = np.delete(around_idx,int(((2*r-1)**2-1)/2),1)\n",
        "\n",
        "################ individual pump well ###################\n",
        "ii = 9\n",
        "\n",
        "head = heads_at_cell[:,ii].reshape((nx,ny))\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(6,6))\n",
        "\n",
        "im = ax.pcolor(X,Y,head)\n",
        "# ax.add_colorbar\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "\n",
        "ax.scatter(X_star[pump_cell_idx[ii],0], X_star[pump_cell_idx[ii],1], zorder=1, alpha= 0.5, c='r', s=10)\n",
        "ax.scatter(X_star[np.delete(pump_cell_idx,ii),0], X_star[np.delete(pump_cell_idx,ii),1], zorder=1, alpha= 0.5, c='b', s=10)\n",
        "ax.scatter(X_star[around_idx[ii],0], X_star[around_idx[ii],1], zorder=1, alpha= 0.7, c='m', s=10)\n",
        "ax.scatter(X_star[xbound_idx,0], X_star[xbound_idx,1], marker='v', zorder=1, alpha= 1, c='r', s=10)\n",
        "ax.scatter(X_star[ybound_idx,0], X_star[ybound_idx,1], marker='v', zorder=1, alpha= 1, c='k', s=10)\n",
        "ax.set_title('Head Tomography')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-PSyntTpzBM"
      },
      "source": [
        "############# data used for all pump tests ##########\n",
        "############# K measurements ##########\n",
        "############# Diri & Neum BCs ##########\n",
        "\n",
        "K_star = K.flatten()[:,None]\n",
        "K_train = K_star[K_measure_id]\n",
        "X_K_train = X_star[K_measure_id]\n",
        "\n",
        "# Domain bounds   \n",
        "lbs = np.array([min(x),min(y)])\n",
        "ubs = np.array([max(x),max(y)])\n",
        "    \n",
        "################### Dirichlet BCs (left & right bound)  ##########\n",
        "N_diri = 64 # No. of point for Dirichlet BCs\n",
        "# left (x=0) and right (x=1)\n",
        "xx = X_star[xbound_idx]\n",
        "uu = np.zeros((xx.shape[0],1))\n",
        "\n",
        "X_diri_train = xx\n",
        "diri_train = uu\n",
        "\n",
        "# X_diri_train, diri_train = random_choice_sample([X_diri_train, diri_train], N_diri)\n",
        "# X_diri_train, diri_train = random_choice_sample([xx2, uu2], N_diri)\n",
        "\n",
        "################### Neumann BCs (top & bottom bound)  #############\n",
        "N_neum = 64  # No. of points for Neumann BCs\n",
        "# bottom (y = 0) and top (y=1)\n",
        "yy = X_star[ybound_idx]\n",
        "du = np.zeros((yy.shape[0],1))\n",
        "\n",
        "X_neum_train = yy\n",
        "neum_train = du\n",
        "\n",
        "# X_neum_train, neum_train = random_choice_sample([X_neum_train, neum_train], N_neum)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w87uKjPuBFXf"
      },
      "source": [
        "hnu = 20 # hidden unit number of net u\n",
        "hnK = 20 # hiddent unit number of net K\n",
        "layers = [2, hnu, hnu, hnu, hnu, hnu, hnu, 1]\n",
        "layers_K = [2, hnK, hnK, hnK, hnK, hnK, hnK, 1]\n",
        "\n",
        "model =PhysicsInformedNN(layers,layers_K)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "io5KtrllfVre"
      },
      "source": [
        "# xlin = np.linspace(0,nx-1,64,dtype=np.int)\n",
        "xlin = np.linspace(0,nx,65,dtype=np.int)\n",
        "xlin[0] = xlin[0]+1\n",
        "xlin[-1] = xlin[-1]-1\n",
        "ylin = np.linspace(0,ny,65,dtype=np.int)\n",
        "ylin[0] = ylin[0]+1\n",
        "ylin[-1] = ylin[-1]-1\n",
        "print(xlin)\n",
        "\n",
        "xlin = np.repeat(xlin,xlin.shape[0])\n",
        "ylin = np.tile(ylin,ylin.shape[0])\n",
        "\n",
        "X_id_65x65  = xlin + ylin*ny\n",
        "# X_f_space = X_star[X_id_65x65]\n",
        "# print(X_f_id.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlBXg2UPLYVF"
      },
      "source": [
        "neum_data = model.data_loader(X_neum_train, neum_train, lbs, ubs)\n",
        "diri_data = model.data_loader(X_diri_train, diri_train, lbs, ubs)\n",
        "K_data = model.data_loader(X_K_train, K_train, lbs, ubs)\n",
        "Qp = 40\n",
        "data_batch = []\n",
        "\n",
        "for jj in range(num_wells):\n",
        "  max_head_at_cell = np.max(np.abs(heads_at_cell[:,jj]))\n",
        "  # print(max_head_at_cell)\n",
        "  heads_at_cell_scaled = heads_at_cell[:,jj][:,None]/max_head_at_cell*0.01\n",
        "  u_star = heads_at_cell_scaled\n",
        "  # print(np.min(u_star))\n",
        "  # pump well and pump rate \n",
        "  X_pump_train = X_star[pump_cell_idx[jj]][None,:]\n",
        "  pump_train = np.array([[Qp]])\n",
        "\n",
        "  # monitor wells and head, besides known BCs\n",
        "  X_u_train = X_star[np.delete(pump_cell_idx,jj)]\n",
        "  u_train = u_star[np.delete(pump_cell_idx,jj)]\n",
        "\n",
        "  # cells outside pump (relax) region set for PDE constraints\n",
        "  region_idx = np.hstack((pump_cell_idx[jj],around_idx[jj],xbound_idx,ybound_idx))\n",
        "\n",
        "  # X_f_id = np.setdiff1d(X_id_65x65,region_idx)\n",
        "  # X_f_space = X_star[X_f_id]\n",
        "  X_f_space = np.delete(X_star, region_idx, 0)\n",
        "  # f_id = np.random.choice(X_f_space.shape[0], 900, replace=False)\n",
        "  X_f_train = X_f_space[::1]\n",
        "  f_train = np.zeros((X_f_train.shape[0],1))\n",
        "\n",
        "  ################ random measurements outside relax region #########\n",
        "  # N_K = 40\n",
        "  # K_measure_space_id = np.delete(space_id, region_idx, 0)\n",
        "  # measure_id = np.random.choice(nx*ny-region_idx.shape[0], N_K, replace=False) \n",
        "  # K_measure_id = K_measure_space_id[measure_id]\n",
        "\n",
        "  \n",
        "  # X_around_train = X_star[around_idx[jj]]\n",
        "  # around_train = u_star[around_idx[jj]]\n",
        "  # around_train = np.zeros((X_around_train.shape[0],1))\n",
        "  # around_data = model.data_loader(X_around_train, around_train, lbs, ubs)\n",
        "  ##############  data around pump wells  ##############\n",
        "  \n",
        "  f_data = model.data_loader(X_f_train, f_train, lbs, ubs)\n",
        "  u_data = model.data_loader(X_u_train, u_train, lbs, ubs)\n",
        "  pump_data = model.data_loader(X_pump_train, pump_train, lbs, ubs)\n",
        "\n",
        "  # Assemble data batch containing training data\n",
        "  # training data is in dict format: key: name -> value: (x,y,val)\n",
        "  train_dict = {\n",
        "    'neum': neum_data,\n",
        "    'diri': diri_data,\n",
        "    'u': u_data,\n",
        "    'f': f_data,\n",
        "    'K': K_data,\n",
        "    'pump': pump_data\n",
        "  }\n",
        "\n",
        "  data_batch.append(train_dict)\n",
        "  \n",
        "fig,ax = plt.subplots(figsize=(7,6))\n",
        "\n",
        "plt.pcolor(X,Y,np.reshape(heads_at_cell[:,-1],[nx,ny]))\n",
        "plt.colorbar()\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "\n",
        "ax.scatter(X_f_train[:,0], X_f_train[:,1], marker='v', zorder=1, alpha= 1, c='k', s=10)\n",
        "print(X_u_train.shape)\n",
        "print(X_f_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJvzFLa_ypgO"
      },
      "source": [
        "# data_batch[pump_id_list[-1]]['u'][2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7O8_SHwuQZi"
      },
      "source": [
        "# pump_id_list = [i for i in range(num_wells)]\n",
        "\n",
        "# X: 12, 0, 4, 20, 24\n",
        "# cross: 12, 2, 10, 14, 22\n",
        "# inner 3x3: 6, 7, 8, 11, 12, 13, 14, 15, 16 \n",
        "# outer 3x3: 12, 2, 10, 14, 22, 0, 4, 20, 24\n",
        "\n",
        "# pump_id_list = [12, 2, 10, 14, 22, 0, 4, 20, 24]\n",
        "# pump_id_list = [0, 2, 4, 10, 12, 14, 20, 22, 24]\n",
        "\n",
        "pump_id_list = [0, 4, 12, 20, 24]\n",
        "\n",
        "# pump_id_list = [24]\n",
        "npump = len(pump_id_list)\n",
        "# npump=5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epLDnV-FhGkI"
      },
      "source": [
        "########## load existing model ##############\n",
        "\n",
        "NK = \"_\"+str(len(K_measure_id))\n",
        "\n",
        "for i in pump_id_list:\n",
        "  params_u = np.loadtxt(save_path+\"/npump_\"+str(npump)+\"/model_u\"+NK+\"_npump_\"+str(npump)+\"_\"+str(i)+\".txt\")\n",
        "  model.weights_u[i], model.biases_u[i] = model.load_NN(layers, params_u)\n",
        "\n",
        "params_K = np.loadtxt(save_path+\"/npump_\"+str(npump)+\"/model_K\"+NK+\"_npump_\"+str(npump)+\"_.txt\")\n",
        "model.weights_K, model.biases_K= model.load_NN(layers_K, params_K)\n",
        "\n",
        "# NK = \"\"\n",
        "# for i in pump_id_list:\n",
        "#   params_u = np.loadtxt(save_path+\"/model_u\"+NK+\"_\"+str(i)+\".txt\")\n",
        "#   model.weights_u[i], model.biases_u[i] = model.load_NN(layers, params_u)\n",
        "\n",
        "# params_K = np.loadtxt(save_path+\"/model_K\"+NK+\".txt\")\n",
        "# model.weights_K, model.biases_K= model.load_NN(layers_K, params_K)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZflB6IIpzBP"
      },
      "source": [
        "# training set up: hyper-parameters\n",
        "\n",
        "# # weights of each kind of loss\n",
        "# loss_weights = {\n",
        "#   'f': 50.0*200,\n",
        "#   'u': 20000.0*200,\n",
        "#   'neum': 10000.0*200,\n",
        "#   'pump': 1.0,\n",
        "#   'K': 100.0*200,\n",
        "#   'diri': 20000.0*200\n",
        "# }\n",
        "\n",
        "# weights of each kind of loss\n",
        "loss_weights = {\n",
        "  'f': 50.0,\n",
        "  'u': 10000.0,\n",
        "  'neum': 10000.0,\n",
        "  'pump': 1.0,\n",
        "  'K': 100.0,\n",
        "  'diri': 20000.0\n",
        "}\n",
        "\n",
        "# loss type aggregated in total loss\n",
        "pred_ks = ['diri', 'neum', 'u', 'pump','f','K']\n",
        "# pred_ks = ['u','diri','neum','K','pump']\n",
        "\n",
        "\n",
        "# networks weights to tune\n",
        "biases_train = []\n",
        "weights_train = []\n",
        "\n",
        "for pid in pump_id_list:\n",
        "  weights_train += model.weights_u[pid]\n",
        "  biases_train += model.biases_u[pid]\n",
        "\n",
        "biases_train += model.biases_K\n",
        "weights_train += model.weights_K\n",
        "\n",
        "# loss change print intervals\n",
        "p_intervals = 300"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvkH9N9mtjnq"
      },
      "source": [
        "# # training process\n",
        "# optimizer = torch.optim.Adam(params=weights_train+biases_train, lr=1e-3)\n",
        "\n",
        "# start_time = time.time()\n",
        "\n",
        "# model.train(901, data_batch, model.loss_func, optimizer, pred_ks, loss_weights, pump_id_list, p_intervals)\n",
        "\n",
        "# # optimizer.lt=1e-5\n",
        "# # model.train(6000, data_batch, model.loss_func, optimizer, pred_ks, loss_weights, pump_id_list, p_intervals)\n",
        "# # optimizer.lt=1e-5\n",
        "# # model.train(6000, data_batch, model.loss_func, optimizer, pred_ks, loss_weights, pump_id_list, p_intervals)\n",
        "\n",
        "# elapsed = time.time() - start_time\n",
        "# print('Training time: %.4f' % (elapsed))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEfb_vVW4FmZ"
      },
      "source": [
        "# # pred_ks = ['diri', 'neum', 'u', 'pump','f','K']\n",
        "# optimizer.lr=1e-6\n",
        "\n",
        "# start_time = time.time()\n",
        "# model.train(10000, data_batch, model.loss_func, optimizer, pred_ks, loss_weights, pump_id_list, 1000)\n",
        "# elapsed = time.time() - start_time\n",
        "# print('Training time: %.4f' % (elapsed))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dClWzqVlV6dn"
      },
      "source": [
        "X_pred= model.coor_shift(X_star, lbs, ubs)\n",
        "error_u_list = []\n",
        "for ti in pump_id_list:\n",
        "\n",
        "  \n",
        "  u_true = heads_at_cell[:,ti]\n",
        "  u_pred = model.predict(X_pred, pid=ti, target='u')/0.01*np.max(np.abs(u_true))\n",
        "  u_err = u_pred-u_true\n",
        "  error_u = np.linalg.norm(u_err,2)/np.linalg.norm(u_true,2)\n",
        "  error_u_list.append(error_u)\n",
        "\n",
        "print(error_u_list)\n",
        "print(\"Min Err: \", min(error_u_list))\n",
        "print(\"Max Err: \", max(error_u_list))\n",
        "print(\"mean Err: \", np.mean(error_u_list))\n",
        "print(\"std Err: \", np.std(error_u_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKiTO1kOpzBR"
      },
      "source": [
        "ti = pump_id_list[2]\n",
        "X_pred= model.coor_shift(X_star, lbs, ubs)\n",
        "u_true = heads_at_cell[:,ti]\n",
        "u_pred = model.predict(X_pred, pid=ti, target='u')/0.01*np.max(np.abs(u_true))\n",
        "\n",
        "u_err = u_pred-u_true\n",
        "error_u = np.linalg.norm(u_err,2)/np.linalg.norm(u_true,2)\n",
        "print('Error u: %e' % (error_u))\n",
        "\n",
        "u_pred = u_pred.reshape((nx,ny))\n",
        "u_true = u_true.reshape((nx,ny))\n",
        "\n",
        "################### head plot: 3D ##################\n",
        "plot_3D(x,y,u_true, 'True')\n",
        "plot_3D(x,y,u_pred, 'Prediction')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiqyVLCxpzBS"
      },
      "source": [
        "################### hydraulic conductivity colormap plot: 2D ##################\n",
        "K_pred = model.predict(X_pred, pid=ti, target='K')\n",
        "K_true = K.flatten()\n",
        "\n",
        "K_err = K_true - K_pred\n",
        "error_K = np.linalg.norm(K_err,2)/np.linalg.norm(K_true,2)\n",
        "print(error_K)\n",
        "K_pred = K_pred.reshape((nx,ny))\n",
        "\n",
        "minlK, maxlK = np.min(logK), np.max(logK)\n",
        "\n",
        "\n",
        "fig,ax = plt.subplots(figsize=(7,6))\n",
        "im = ax.pcolor(X,Y,logK,cmap=cmap5[13])\n",
        "im.set_clim(minlK, maxlK )\n",
        "fig.colorbar(im, ax=ax)\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "ax.scatter(X_star[K_measure_id,0], X_star[K_measure_id,1], zorder=1, alpha= 0.8, c='r', s=15)\n",
        "ax.set_title('True')\n",
        "logK_pred = np.nan_to_num(np.log(K_pred),nan=minlK)\n",
        "# K_pred = np.log(K_pred)\n",
        "fig,ax = plt.subplots(figsize=(7,6))\n",
        "im = ax.pcolor(X,Y, logK_pred,cmap=cmap5[13])\n",
        "im.set_clim(minlK, maxlK )\n",
        "\n",
        "fig.colorbar(im, ax=ax)\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "ax.scatter(X_star[K_measure_id,0], X_star[K_measure_id,1], zorder=1, alpha= 0.8, c='r', s=15)\n",
        "ax.set_title('Prediction')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfo4LlLzEmyS"
      },
      "source": [
        "fig,ax = plt.subplots(figsize=(7,6))\n",
        "\n",
        "# threshold 10%\n",
        "thres = 0.1\n",
        "K_len = maxlK-minlK\n",
        "\n",
        "# acc = np.divide(abs(logK-K_pred),abs(K))\n",
        "acc = abs(logK-logK_pred)/K_len\n",
        "\n",
        "\n",
        "print(sum(sum(acc<thres))/(nx*ny))\n",
        "plt.pcolor(X,Y,acc<thres,cmap=plt.cm.BuPu_r)\n",
        "\n",
        "plt.colorbar()\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.title('Absolute Error')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lS79E1PisJab"
      },
      "source": [
        "################### head countour plot: 2D ##################\n",
        "\n",
        "scaler_min = np.min(u_true)\n",
        "scaler_max = np.max(u_true)\n",
        "scaler_len = scaler_max - scaler_min\n",
        "# print(scaler_len)\n",
        "\n",
        "u_scaled = (u_pred - np.min(u_pred))/(np.max(u_pred) - np.min(u_pred))\n",
        "u_scaled = u_scaled * scaler_len+scaler_min\n",
        "\n",
        "mask_min = np.min(u_scaled)\n",
        "# print(mask_min)\n",
        "# Exact = u_true\n",
        "# Exact[Exact<mask_min] = mask_min\n",
        "\n",
        "# plot_3D(x,y,u_scaled, 'Prediction')\n",
        "# plot_3D(x,y,Exact, 'True')\n",
        "\n",
        "# compare_true_pred(Exact, u_scaled, x, y)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,5))\n",
        "lvls = np.linspace(scaler_min,scaler_max,7)\n",
        "\n",
        "CT = ax1.contour(X, Y, u_true, levels=lvls)\n",
        "ax1.clabel(CT,inline=True)\n",
        "ax1.set_title('True')\n",
        "\n",
        "CP = ax2.contour(X, Y, u_pred,levels=lvls,linestyles='dashed')\n",
        "ax2.clabel(CP,inline=True)\n",
        "ax2.set_title('Prediction')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zBte7dm1CZ1"
      },
      "source": [
        "################### quiver (1st-order gradient arrow) plot: 2D ##################\n",
        "# dx, dy = 5.0, 5.0\n",
        "\n",
        "du_true_cen = (u_true[:,2:] - u_true[:,0:-2])/dx/2\n",
        "du_true_left = (u_true[:,1:2] - u_true[:,0:1])/dx\n",
        "du_true_right = (u_true[:,-1:] - u_true[:,-2:-1])/dx\n",
        "du_true_dx = np.hstack((du_true_left, du_true_cen, du_true_right))\n",
        "\n",
        "du_true_mid = (u_true[2:] - u_true[0:-2,])/dy/2\n",
        "du_true_top = (u_true[1:2] - u_true[0:1])/dy\n",
        "du_true_bot = (u_true[-1:] - u_true[-2:-1])/dy\n",
        "du_true_dy = np.vstack((du_true_top, du_true_mid, du_true_bot))\n",
        "\n",
        "dudx, dudy = model.predict(X_pred, pid=ti, target='du')\n",
        "error_neum = np.linalg.norm(dudy[ybound_idx],2)/dudy[ybound_idx].shape[0]\n",
        "print('Error neum: %e' % (error_neum))\n",
        "\n",
        "dudx = dudx.reshape((nx,ny))\n",
        "dudy = dudy.reshape((nx,ny))\n",
        "fig2, (ax3, ax4) = plt.subplots(1, 2, figsize=(13,5))\n",
        "\n",
        "im2 = ax3.pcolor(X,Y,u_true)\n",
        "cbar = fig2.colorbar(im2, ax=(ax3, ax4), shrink=0.95)\n",
        "\n",
        "ax3.quiver(X,Y,du_true_dx, du_true_dy)\n",
        "ax3.set_title('True')\n",
        "\n",
        "ax4.pcolor(X,Y,u_pred)\n",
        "ax4.quiver(X,Y,dudx, dudy)\n",
        "ax4.set_title('Prediction')\n",
        "# im2 = ax4.pcolor(X,Y,u_true)\n",
        "# cbar = fig2.colorbar(im2, ax=(ax3, ax4), shrink=0.95)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qllrmtc5p98g"
      },
      "source": [
        "################### f PDE (2nd-order derivatives) plot: 2D & 3D ##################\n",
        "du2_cen = ((u_true[:,2:] - u_true[:,1:-1]) - (u_true[:,1:-1] - u_true[:,0:-2]))/dx**2\n",
        "du2_left = du2_cen[:,0][:,None]\n",
        "du2_right = du2_cen[:,-1][:,None]\n",
        "du2dx = np.hstack((du2_left, du2_cen, du2_right))\n",
        "\n",
        "du2_mid = ((u_true[2:] - u_true[1:-1]) - (u_true[1:-1] - u_true[0:-2]))/dy**2\n",
        "du2_top = du2_mid[0]\n",
        "du2_bot = du2_mid[-1]\n",
        "du2dy = np.vstack((du2_top, du2_mid, du2_bot))\n",
        "\n",
        "dK_true_cen = (K[:,2:] - K[:,0:-2])/dx/2\n",
        "dK_true_left = (K[:,1:2] - K[:,0:1])/dx\n",
        "dK_true_right = (K[:,-1:] - K[:,-2:-1])/dx\n",
        "dK_true_dx = np.hstack((dK_true_left, dK_true_cen, dK_true_right))\n",
        "\n",
        "dK_true_mid = (K[2:] - K[0:-2,])/dy/2\n",
        "dK_true_top = (K[1:2] - K[0:1])/dy\n",
        "dK_true_bot = (K[-1:] - K[-2:-1])/dy\n",
        "dK_true_dy = np.vstack((dK_true_top, dK_true_mid, dK_true_bot))\n",
        "\n",
        "f_res = np.multiply(dK_true_dx,du_true_dx) + np.multiply(dK_true_dy, du_true_dy) + np.multiply(du2dy+du2dx,K)\n",
        "\n",
        "f_pred = model.predict(X_pred, pid=ti, target='f')\n",
        "\n",
        "region_idx = np.unique(np.hstack((pump_cell_idx[ti],around_idx[ti],xbound_idx,ybound_idx)))\n",
        "f_err = np.delete(f_pred, region_idx, 0)\n",
        "error_pde = np.linalg.norm(f_err,2)/f_err.shape[0]\n",
        "print('Error pde: %e' % (error_pde))\n",
        "\n",
        "f_pred = f_pred.reshape((nx,ny))\n",
        "\n",
        "fig3, (ax5, ax6) = plt.subplots(1, 2, figsize=(12,5))\n",
        "ax5.pcolor(X,Y,f_pred)\n",
        "ax5.set_title('Prediction')\n",
        "ax6.pcolor(X,Y,f_res)\n",
        "ax6.set_title('True')\n",
        "\n",
        "ax5.scatter(X_star[around_idx[ti],0], X_star[around_idx[ti],1], zorder=1, alpha= 0.6, c='y', s=10)\n",
        "ax6.scatter(X_star[around_idx[ti],0], X_star[around_idx[ti],1], zorder=1, alpha= 0.6, c='y', s=10)\n",
        "\n",
        "\n",
        "plot_3D(x,y,f_pred,figname='prediction')\n",
        "plot_3D(x,y,f_res,figname='true')\n",
        "print(np.max(f_res))\n",
        "print(np.min(f_res))\n",
        "print(Q/dx/dy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYdlmsjf1REY"
      },
      "source": [
        "# ############## save well-trained model: u, K, and measured K id\n",
        "# if not isDir:\n",
        "#   try:\n",
        "#     os.mkdir(save_path)\n",
        "#     os.mkdir(save_path+\"/npump_\"+str(npump))\n",
        "#   except:\n",
        "#     FileExistsError\n",
        "\n",
        "# NK = \"_\"+str(len(K_measure_id))\n",
        "# m = model\n",
        "# for i in pump_id_list:\n",
        "#   weights = torch.ones(1,hnu)*i\n",
        "\n",
        "#   for j in range(len(layers)-2):\n",
        "#     w = m.weights_u[i][j]\n",
        "#     b = m.biases_u[i][j]\n",
        "#     weights = torch.vstack((weights,w,b))\n",
        "\n",
        "#   weights = torch.vstack((weights,m.weights_u[i][j+1].T,torch.ones(1,hnu)*m.biases_u[i][j+1]))\n",
        "#   weights=weights.detach().numpy()\n",
        "\n",
        "\n",
        "#   path_name = save_path+\"/npump_\"+str(npump)+\"/model_u\"+NK+\"_npump_\"+str(len(pump_id_list))+\"_\"+str(i)+\".txt\"\n",
        "#   np.savetxt(path_name,weights)\n",
        "\n",
        "# weights = torch.ones(1,hnK)*i\n",
        "\n",
        "# for j in range(len(layers)-2):\n",
        "#   w = m.weights_K[j]\n",
        "#   b = m.biases_K[j]\n",
        "#   weights = torch.vstack((weights,w,b))\n",
        "\n",
        "# weights = torch.vstack((weights,m.weights_K[j+1].T,torch.ones(1,hnK)*m.biases_K[j+1]))\n",
        "# weights=weights.detach().numpy()\n",
        "\n",
        "# path_name = save_path+\"/npump_\"+str(npump)+\"/model_K\"+NK+\"_npump_\"+str(len(pump_id_list))+\"_\"+\".txt\"\n",
        "# np.savetxt(path_name,weights)\n",
        "\n",
        "# path_name = save_path+\"/npump_\"+str(npump)+\"/K_measure_id\"+NK+\".txt\"\n",
        "# np.savetxt(path_name,K_measure_id.astype(int))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBLmo_QDvngy"
      },
      "source": [
        "# ############## save hyper-parameters for reproduce ##################\n",
        "# hyper_param = copy.copy(loss_weights)\n",
        "# hyper_param['Qp'] = Qp\n",
        "# hyper_param['r'] = r\n",
        "# hyper_param['hnK'] = hnK\n",
        "# hyper_param['hnu'] = hnu\n",
        "# hyper_param['lenK'] = len(layers_K)\n",
        "# hyper_param['lenu'] = len(layers)\n",
        "# hyper_param['act_K'] = 'tanh'\n",
        "# hyper_param['act_u'] = 'tanh'\n",
        "# hyper_param['out_K'] = 'linear'\n",
        "# hyper_param['out_u'] = 'linear'\n",
        "\n",
        "# f = open(save_path+\"hyper_parameters.txt\",\"w\")\n",
        "# f.write(str(hyper_param))\n",
        "# f.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}